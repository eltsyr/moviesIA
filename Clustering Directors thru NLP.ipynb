{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORTANT : si l'execution déclenche une erreur et demande nltk.download, \n",
    "# faire executer ces 2 lignes : \n",
    "# nltk.download('punkt')\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "import nltk\n",
    "\n",
    "#---------------------------------------------\n",
    "# FONCTION\n",
    "# input : director name (case insensitive)\n",
    "# output : cleaned article retrieved\n",
    "#---------------------------------------------\n",
    "def GetWikipediaSummary(director):\n",
    "    # Récupération de la page exacte wikipedia (après redirection) à partir du nom de l'auteur non normalisé\n",
    "    director.replace(\" \",\"+\")\n",
    "    mysearchURL = \"https://en.wikipedia.org/w/index.php?search=\"+director\n",
    "    mysummaryURL = \"https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&redirects=true&exintro&explaintext&titles=\"\n",
    "    http = requests.get(mysearchURL)\n",
    "    #On vire les 30 premiers caractères pour trouver le nom normalisé de l'auteur\n",
    "    exactURL = mysummaryURL + http.url[30:]\n",
    "    #Récupération du résumé\n",
    "    fullresults= requests.get(exactURL).text\n",
    "    summaryindex = fullresults.find(\"extract\")\n",
    "    summary =fullresults[summaryindex+10:-5]\n",
    "\n",
    "    #CLEANUP\n",
    "    summary = summary.lower()\n",
    "    \n",
    "    #On vire les requetes qui n'ont rien donné sur wikipedia\n",
    "    if summary.find(\"may refer\")!=-1:\n",
    "        return \"\"\n",
    "    if summary.find(\"normalized\")!=-1:\n",
    "        return \"\"\n",
    "    \n",
    "    #on vire les caractères spéciaux & les chiffres & les slashs\n",
    "    summary.encode('ascii', 'replace')\n",
    "    summary=re.sub(\"[0-9\\\\\\\\]+\",\"\",summary)\n",
    "    \n",
    "    #On vire le nom/prénom du directeur\n",
    "    for director_part in director.lower().split(\" \"):\n",
    "        summary = summary.replace(director_part,\"\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Le plan\n",
    "# 1. Récupérer pour les 250 réalisateurs les mieux notés en moyenne\n",
    "# 2. Appliquer une pondération TF-IDF (Term Frequency / Inverse Document Frequency)\n",
    "# 3. Lancer un algo de clustering\n",
    "\n",
    "#Step 1\n",
    "df = pd.read_csv(\"favorite_directors.csv\", sep=\";\", low_memory=False, names = [\"director_name\"])\n",
    "df.set_index(\"director_name\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on vire les index dupliqués éventuellement\n",
    "df = df[~df.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving (0/220) summary for : Werner Herzog\n",
      "Retrieving (1/220) summary for : François Truffaut\n",
      "Retrieving (2/220) summary for : Fritz Lang\n",
      "Retrieving (3/220) summary for : Eric Rohmer\n",
      "Retrieving (4/220) summary for : Akira Kurosawa\n",
      "Retrieving (5/220) summary for : Alfred Hitchcock\n",
      "Retrieving (6/220) summary for : Luis Buñuel\n",
      "Retrieving (7/220) summary for : Howard Hawks\n",
      "Retrieving (8/220) summary for : Ingmar Bergman\n",
      "Retrieving (9/220) summary for : Charles Chaplin\n",
      "Retrieving (10/220) summary for : Robert Bresson\n",
      "Retrieving (11/220) summary for : Michael Powell & Emeric Pressburger\n",
      "Retrieving (12/220) summary for : Alain Resnais\n",
      "Retrieving (13/220) summary for : Joseph Losey\n",
      "Retrieving (14/220) summary for : Kinji Fukasuku\n",
      "Retrieving (15/220) summary for : Josef von Sternberg\n",
      "Retrieving (16/220) summary for : Billy Wilder\n",
      "Retrieving (17/220) summary for : Joseph L. Mankiewicz\n",
      "Retrieving (18/220) summary for : Agnès Varda\n",
      "Retrieving (19/220) summary for : Roberto Rossellini\n",
      "Retrieving (20/220) summary for : Mamoru Oshii\n",
      "Retrieving (21/220) summary for : Krzysztof Kieślowski\n",
      "Retrieving (22/220) summary for : Roman Polanski\n",
      "Retrieving (23/220) summary for : Sidney Lumet\n",
      "Retrieving (24/220) summary for : Joon-ho Bong\n"
     ]
    }
   ],
   "source": [
    "#Step 2\n",
    "for x in range(len(df)):\n",
    "    if type(df.iloc[x].name)!=str:\n",
    "        df.loc[df.iloc[x].name,\"summary\"]=\"\"\n",
    "    else:\n",
    "        summary = GetWikipediaSummary(df.iloc[x].name)\n",
    "        print(\"Retrieving (\"+str(x)+ \"/\" + str(len(df)) +\") summary for : \"+df.iloc[x].name)\n",
    "        df.loc[df.iloc[x].name,\"summary\"]=summary\n",
    "#On vire les entrées sans résumé\n",
    "df_backup = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_backup\n",
    "#on vire toutes les entrées sans résumé\n",
    "df_final = df[df[\"summary\"]!=\"\"].copy()\n",
    "#On transforme en liste\n",
    "liste = list(df[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 1582.508\n",
      "Iteration  1, inertia 781.204\n",
      "Iteration  2, inertia 775.801\n",
      "Iteration  3, inertia 773.370\n",
      "Iteration  4, inertia 772.049\n",
      "Iteration  5, inertia 770.249\n",
      "Iteration  6, inertia 769.187\n",
      "Iteration  7, inertia 768.788\n",
      "Iteration  8, inertia 768.638\n",
      "Iteration  9, inertia 768.628\n",
      "Converged at iteration 9: center shift 0.000000e+00 within tolerance 2.039296e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       " array([143,  84,  33,  94,  41, 276,  83,  60,  26,  14,  98,  40], dtype=int64))"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On clusterise\n",
    "nb_cluster = 12\n",
    "#On applique la transformation TFIDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,min_df=2,stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(liste)\n",
    "km = KMeans(n_clusters = nb_cluster ,init=\"k-means++\",max_iter=100,n_init=1,verbose=True)\n",
    "km.fit(X)\n",
    "np.unique(km.labels_,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère les mots les plus courants dans chaque cluster\n",
    "#Step 1 : récupération du texte total de tous les clusters\n",
    "text={}\n",
    "for i,cluster in enumerate(km.labels_):\n",
    "    oneDoc = liste[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster]=oneDoc\n",
    "    else:\n",
    "        text[cluster]+=oneDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 : décompte du nb d'occurence de tous les mots et on garde les 100 plus fréquents par cluster\n",
    "_stopwords = set (stopwords.words(\"english\")+list(punctuation)+[\"film\"])\n",
    "keywords={}\n",
    "counts={}\n",
    "for cluster in range (nb_cluster):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster]=nlargest(100,freq,key=freq.get)\n",
    "    counts[cluster]=freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3 : on ne garde que les mots d'un cluster qui ne sont pas présents dans les autres clusters\n",
    "unique_keys={}\n",
    "for cluster in range(nb_cluster):\n",
    "    other_clusters=list(set(range(nb_cluster))-set([cluster]))\n",
    "    keys_other_clusters=set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "    unique=set(keywords[cluster])-keys_other_clusters\n",
    "    unique_keys[cluster]=nlargest(10,unique,key=counts[cluster].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['series',\n",
       "  'horror',\n",
       "  'writing',\n",
       "  'well',\n",
       "  'sequel',\n",
       "  'company',\n",
       "  'thriller',\n",
       "  'british',\n",
       "  'dead',\n",
       "  'superhero'],\n",
       " 1: ['french',\n",
       "  'wave',\n",
       "  'les',\n",
       "  'le',\n",
       "  'la',\n",
       "  'critics',\n",
       "  'de',\n",
       "  'paris',\n",
       "  'january',\n",
       "  'considered'],\n",
       " 2: ['effects',\n",
       "  'special',\n",
       "  'use',\n",
       "  'life',\n",
       "  'movies',\n",
       "  'united',\n",
       "  'animated',\n",
       "  'hollywood',\n",
       "  'part',\n",
       "  'disney'],\n",
       " 3: ['awards',\n",
       "  'became',\n",
       "  'critical',\n",
       "  'york',\n",
       "  '.n',\n",
       "  'years',\n",
       "  'art',\n",
       "  'success',\n",
       "  'hollywood',\n",
       "  'nominated'],\n",
       " 4: ['polish',\n",
       "  'international',\n",
       "  'years',\n",
       "  'poland',\n",
       "  'awards',\n",
       "  'became',\n",
       "  'prize',\n",
       "  'soviet',\n",
       "  'entered',\n",
       "  'life'],\n",
       " 5: ['hong',\n",
       "  'kong',\n",
       "  'chinese',\n",
       "  'martial',\n",
       "  'german',\n",
       "  'russian',\n",
       "  'romanian',\n",
       "  'arts',\n",
       "  'thai',\n",
       "  'written'],\n",
       " 6: ['awards',\n",
       "  'nominated',\n",
       "  'screenplay',\n",
       "  'golden',\n",
       "  'nominations',\n",
       "  'original',\n",
       "  'globe',\n",
       "  'bafta',\n",
       "  'oscar',\n",
       "  'winning'],\n",
       " 7: ['cannes',\n",
       "  'international',\n",
       "  'prize',\n",
       "  'jury',\n",
       "  \"d'or\",\n",
       "  'palme',\n",
       "  'romanian',\n",
       "  'th',\n",
       "  'berlin',\n",
       "  'golden'],\n",
       " 8: ['chinese',\n",
       "  'wang',\n",
       "  'forestry',\n",
       "  'based',\n",
       "  'minister',\n",
       "  'russian',\n",
       "  'generation',\n",
       "  'ice',\n",
       "  'notably',\n",
       "  'figures'],\n",
       " 9: ['korean',\n",
       "  'south',\n",
       "  'vengeance',\n",
       "  'hangul',\n",
       "  'pronunciation',\n",
       "  'cannes',\n",
       "  'humor',\n",
       "  'subject',\n",
       "  'noted',\n",
       "  'lion'],\n",
       " 10: ['awards',\n",
       "  'hollywood',\n",
       "  'crime',\n",
       "  'nominated',\n",
       "  'years',\n",
       "  'nominations',\n",
       "  'became',\n",
       "  'golden',\n",
       "  'life',\n",
       "  'early'],\n",
       " 11: ['japanese',\n",
       "  'japan',\n",
       "  'uu',\n",
       "  'manga',\n",
       "  'studio',\n",
       "  'working',\n",
       "  'awards',\n",
       "  'anime',\n",
       "  'animation',\n",
       "  'palme']}"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"labels\"]=km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ecriture dans un fichier\n",
    "df.to_csv(\"temp.csv\",encoding='utf-8', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
